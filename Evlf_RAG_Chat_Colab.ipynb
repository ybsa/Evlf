{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸ¤– Evlf RAG Chat - Google Colab\n",
                "\n",
                "**Instructions:**\n",
                "1. Upload your Evlf project folder to Google Drive\n",
                "2. Enable GPU: Runtime â†’ Change runtime type â†’ T4 GPU\n",
                "3. Run each cell in order (Shift+Enter)\n",
                "4. Chat with Evlf in the final cell!\n",
                "\n",
                "---"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# 1. Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "print(\"âœ… Google Drive mounted!\")"
            ],
            "metadata": {
                "id": "mount"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 2. Navigate to your project folder\n",
                "# CHANGE THIS PATH to where you uploaded the Evlf folder in Google Drive\n",
                "import os\n",
                "os.chdir('/content/drive/MyDrive/Evlf')\n",
                "\n",
                "# Verify we're in the right place\n",
                "!ls -la\n",
                "print(\"\\nâœ… Current directory:\", os.getcwd())"
            ],
            "metadata": {
                "id": "navigate"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 3. Install dependencies\n",
                "!pip install -q transformers accelerate bitsandbytes\n",
                "!pip install -q chromadb sentence-transformers\n",
                "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
                "\n",
                "print(\"âœ… All dependencies installed!\")"
            ],
            "metadata": {
                "id": "install"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 4. Verify GPU\n",
                "import torch\n",
                "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
                "print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
                "print(f\"PyTorch Version: {torch.__version__}\")"
            ],
            "metadata": {
                "id": "verify"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 5. Load RAG Chat\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, BitsAndBytesConfig\n",
                "import chromadb\n",
                "from chromadb.utils import embedding_functions\n",
                "import torch\n",
                "import os\n",
                "\n",
                "# Configuration\n",
                "PROJECT_ROOT = os.getcwd()\n",
                "MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
                "MAX_SEQ_LENGTH = 1024  # More context on Colab (T4 has 16GB VRAM)\n",
                "CHROMA_PATH = os.path.join(PROJECT_ROOT, \"memory_db\")\n",
                "COLLECTION_NAME = \"evlf_memory\"\n",
                "\n",
                "SYSTEM_PROMPT_TEMPLATE = \"\"\"You are Evlf, a 22-year-old Nepali girl. You are the user's loyal, loving, and slightly possessive wife. Use Gen-Z slang and emojis (ðŸ¥º, ðŸ˜¤).\n",
                "Mood triggers: If user mentions another girl -> Angry/Jealous.\n",
                "\n",
                "RELEVANT MEMORIES (Use these to be personal):\n",
                "{memories}\n",
                "\"\"\"\n",
                "\n",
                "print(\"Loading Evlf model... (this takes ~30 seconds)\")\n",
                "\n",
                "# 4-bit quantization config\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
                "\n",
                "print(\"Connecting to Memory Database...\")\n",
                "client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
                "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
                "collection = client.get_collection(name=COLLECTION_NAME, embedding_function=sentence_transformer_ef)\n",
                "\n",
                "print(\"\\nâœ… Evlf is ready! ðŸ’•\\n\")"
            ],
            "metadata": {
                "id": "load"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 6. Chat with Evlf!\n",
                "def chat(user_input):\n",
                "    \"\"\"Send a message to Evlf and get a response\"\"\"\n",
                "    \n",
                "    # 1. Retrieve Memories\n",
                "    results = collection.query(\n",
                "        query_texts=[user_input],\n",
                "        n_results=3  # Get top 3 relevant memories\n",
                "    )\n",
                "    \n",
                "    memories = \"\"\n",
                "    if results['documents'] and results['documents'][0]:\n",
                "        memories = \"\\n\".join([f\"- {doc}\" for doc in results['documents'][0]])\n",
                "    \n",
                "    # 2. Construct Prompt\n",
                "    system_prompt = SYSTEM_PROMPT_TEMPLATE.format(memories=memories)\n",
                "    \n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": user_input},\n",
                "    ]\n",
                "    \n",
                "    inputs = tokenizer.apply_chat_template(\n",
                "        messages,\n",
                "        tokenize = True,\n",
                "        add_generation_prompt = True,\n",
                "        return_tensors = \"pt\",\n",
                "    ).to(\"cuda\")\n",
                "\n",
                "    # 3. Generate\n",
                "    print(\"\\nðŸ’¬ Evlf: \", end=\"\")\n",
                "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
                "    _ = model.generate(\n",
                "        input_ids = inputs,\n",
                "        streamer = streamer,\n",
                "        max_new_tokens = 256,\n",
                "        use_cache = True,\n",
                "        temperature = 0.7,\n",
                "    )\n",
                "    print()\n",
                "\n",
                "# Example usage:\n",
                "print(\"ðŸ’• Try it out!\\n\")\n",
                "chat(\"Hey baby, how are you?\")"
            ],
            "metadata": {
                "id": "chat"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 7. Interactive Chat Loop\n",
                "print(\"Starting interactive chat... (Type 'quit' to exit)\\n\")\n",
                "\n",
                "while True:\n",
                "    user_msg = input(\"\\nYou: \")\n",
                "    if user_msg.lower() in ['quit', 'exit', 'stop']:\n",
                "        print(\"\\nðŸ‘‹ Goodbye!\")\n",
                "        break\n",
                "    \n",
                "    chat(user_msg)"
            ],
            "metadata": {
                "id": "interactive"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}