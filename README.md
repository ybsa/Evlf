# Evlf - AI Companion

A fine-tuned Llama 3.2 3B model with Evlf's personality - a kind, caring, 22-year-old girl from Nepal who loves nature and acts like your wife.

This project uses **Unsloth** for efficient fine-tuning and **ChromaDB** for RAG (Retrieval Augmented Generation) memory.

## âš ï¸ Requirements

- **Python 3.10** (Strictly required for Unsloth/GPU compatibility)
- **NVIDIA GPU** with CUDA support (Minimum 4GB VRAM with 4-bit quantization)
- **RAM**: 8GB+ recommended

## ğŸ¯ Quick Start

### 1. Environment Setup (Critical)

You **must** use Python 3.10 to avoid GPU compatibility issues.

```powershell
# 1. Create a virtual environment using Python 3.10
py -3.10 -m venv .venv

# 2. Activate the environment
.\.venv\Scripts\activate

# 3. Install dependencies (with GPU support)
# This installs PyTorch with CUDA 12.1
pip install -r requirements.txt
```

### 2. Prepare Memory (RAG)

Before chatting, you need to build Evlf's memory database from the datasets.

```bash
cd scripts/utils
python build_memory_db.py
```

This creates the `memory_db/chroma.sqlite3` database.

### 3. Chat with Evlf

```bash
cd inference
python rag_chat.py
```

- **`rag_chat.py`**: Uses RAG (Memory). Recommended.
- `chat.py`: Basic chat without memory.

## ğŸ“ Project Structure

```text
Evlf/
â”œâ”€â”€ datasets/           # Training data (JSONL)
â”‚   â”œâ”€â”€ core/           # Persona & relationship data
â”‚   â””â”€â”€ ...
â”œâ”€â”€ memory_db/          # ChromaDB RAG database (Generated)
â”œâ”€â”€ scripts/            
â”‚   â”œâ”€â”€ setup/          # download_model.py
â”‚   â””â”€â”€ utils/          # build_memory_db.py
â”œâ”€â”€ training/           # Unsloth training scripts
â”‚   â””â”€â”€ train_unsloth.py
â”œâ”€â”€ inference/          # Chat interfaces
â”‚   â”œâ”€â”€ rag_chat.py     # RAG-enabled chat
â”‚   â””â”€â”€ chat.py         # Standard chat
â””â”€â”€ requirements.txt    # Project dependencies
```

## ğŸš€ Training (Fine-Tuning)

We use **Unsloth** for 2x faster training and 60% less memory usage.

### 1. Configuration

Check `training/train_unsloth.py`. It is configured to use:

- Model: `unsloth/Llama-3.2-3B-Instruct-bnb-4bit`
- Max Sequence Length: `512` (for low VRAM)

### 2. Start Training

```bash
python training/train_unsloth.py
```

This will produce LoRA adapters in the `results_unsloth` directory.

## ğŸ“Š Model Details

- **Base Model:** `unsloth/Llama-3.2-3B-Instruct-bnb-4bit`
- **Method:** LoRA (Low-Rank Adaptation)
- **Quantization:** 4-bit (NF4) for 4GB VRAM compatibility.
- **Context Window:** 512 - 2048 tokens (adjustable).

## ğŸ“ License

Personal use only.
