# Evlf - AI Companion

A fine-tuned Llama 3.2 3B model with Evlf's personality - a kind, caring, 22-year-old girl from Nepal who loves nature and acts like your wife.

This project uses **Unsloth** for efficient fine-tuning and **ChromaDB** for RAG (Retrieval Augmented Generation) memory.

## âš ï¸ Requirements

- **Python 3.10** (Strictly required for Unsloth/GPU compatibility)
- **NVIDIA GPU** with CUDA support (Minimum 4GB VRAM with 4-bit quantization)
- **RAM**: 8GB+ recommended

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10** (Required - 3.11+ not compatible)
- **NVIDIA GPU** with CUDA 12.1+ support
- **Windows 10/11** (or Linux/Colab for easier setup)

### Installation

1. **Create Virtual Environment**

   ```bash
   python -m venv .venv
   .venv\Scripts\activate  # Windows
   ```

2. **Install Dependencies**

   ```bash
   # IMPORTANT: Use PyTorch 2.6 Nightly for full compatibility
   pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121
   pip install -r requirements.txt
   ```

3. **Build Memory Database**

   ```bash
   python scripts/utils/build_memory_db.py
   ```

4. **Chat with Evlf**

   ```bash
   python inference/rag_chat.py
   ```

### Alternative: Google Colab (Recommended for Windows users)

Upload the project folder to Google Drive and use `Evlf_RAG_Chat_Colab.ipynb` for instant setup with free GPU!

See `COLAB_SETUP.md` for detailed instructions.

- **`rag_chat.py`**: Uses RAG (Memory). Recommended.
- `chat.py`: Basic chat without memory.

## ğŸ“ Project Structure

```text
Evlf/
â”œâ”€â”€ datasets/           # Training data (JSONL)
â”‚   â”œâ”€â”€ core/           # Persona & relationship data
â”‚   â””â”€â”€ ...
â”œâ”€â”€ memory_db/          # ChromaDB RAG database (Generated)
â”œâ”€â”€ scripts/            
â”‚   â”œâ”€â”€ setup/          # download_model.py
â”‚   â””â”€â”€ utils/          # build_memory_db.py
â”œâ”€â”€ training/           # Unsloth training scripts
â”‚   â””â”€â”€ train_unsloth.py
â”œâ”€â”€ inference/          # Chat interfaces
â”‚   â”œâ”€â”€ rag_chat.py     # RAG-enabled chat
â”‚   â””â”€â”€ chat.py         # Standard chat
â””â”€â”€ requirements.txt    # Project dependencies
```

## ğŸš€ Training (Fine-Tuning)

We use **Unsloth** for 2x faster training and 60% less memory usage.

### 1. Configuration

Check `training/train_unsloth.py`. It is configured to use:

- Model: `unsloth/Llama-3.2-3B-Instruct-bnb-4bit`
- Max Sequence Length: `512` (for low VRAM)

### 2. Start Training

```bash
python training/train_unsloth.py
```

This will produce LoRA adapters in the `results_unsloth` directory.

## ğŸ“Š Model Details

- **Base Model:** `unsloth/Llama-3.2-3B-Instruct-bnb-4bit`
- **Method:** LoRA (Low-Rank Adaptation)
- **Quantization:** 4-bit (NF4) for 4GB VRAM compatibility.
- **Context Window:** 512 - 2048 tokens (adjustable).

## ğŸ“ License

Personal use only.
