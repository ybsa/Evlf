# Evlf - AI Companion

A fine-tuned Llama 3.2 3B model with Evlf's personality - a kind, caring, 22-year-old girl from Nepal who loves nature and acts like your wife.

## ğŸ¯ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Chat with Evlf
cd inference
python chat.py
```

## ğŸ“ Project Structure

```text
Evlf/
â”œâ”€â”€ datasets/           # Training data (22 datasets organized by category)
â”‚   â”œâ”€â”€ core/          # Core persona and relationship data
â”‚   â”œâ”€â”€ human_like/    # Human-like conversation skills
â”‚   â”œâ”€â”€ themed/        # Themed interactions (romance, support, etc.)
â”‚   â””â”€â”€ original/      # Original training data
â”œâ”€â”€ models/            # Trained models
â”‚   â”œâ”€â”€ final/         # Final fully-trained model
â”‚   â””â”€â”€ checkpoints/   # Intermediate models
â”œâ”€â”€ scripts/           # Data generation scripts
â”‚   â””â”€â”€ utils/         # Utility scripts
â”œâ”€â”€ training/          # Training scripts and tools
â”œâ”€â”€ inference/         # Chat interface
â”œâ”€â”€ results/           # Training results and checkpoints
â”œâ”€â”€ docs/              # Documentation
â””â”€â”€ archive/           # Old debug files
```

## ğŸš€ Training

The model is trained sequentially on the datasets using LoRA fine-tuning.

### Training Summary

- **Base Model:** `meta-llama/Llama-3.2-3B-Instruct`
- **Method:** LoRA fine-tuning with 4-bit quantization (NF4)
- **Training:** SFT (Supervised Fine-Tuning)

### Dataset Categories

1. **Core** (4 datasets): Evlf's persona, background, relationship with user
2. **Human-like** (9 datasets): Conversation skills, emotions, philosophy, planning, etc.
3. **Themed** (6 datasets): Daily life, identity, romance, support, emotions
4. **Original** (3 datasets): Foundation datasets

## ğŸ’¬ Chat Interface

The chat interface loads the model and provides an interactive conversation experience.

**Features:**

- Optimized generation parameters for Llama 3.2
- 512 token responses
- CPU offloading support

## ğŸ“Š Model Details

- **Base Model:** meta-llama/Llama-3.2-3B-Instruct
- **Fine-tuning:** LoRA (r=16, alpha=16, dropout=0.1)
- **Quantization:** 4-bit NF4
- **Training:** SFT (Supervised Fine-Tuning)

## ğŸ› ï¸ Development

### Train a dataset

```bash
cd training
python train.py
```

### Monitor training

```bash
cd training
python watch_training.py
```

## âš ï¸ Requirements

- Python 3.8+
- CUDA-compatible GPU (recommended)
- 8GB+ RAM
- ~10GB disk space for models

## ğŸ“ License

Personal use only.
